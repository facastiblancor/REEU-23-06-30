{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REEU-23-06-30 CAN BUS J1939 Data Wrangling Session\n",
    "### By Fabio A. Castiblanco, based on Dr. Yang Wang's CAN BUS Reverse Engineering and Parsing code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## What is a \"CAN BUS\"?\n",
    "\n",
    "A **Controller Area Network** (CAN bus) is a vehicle bus standard designed to **allow microcontrollers and devices to communicate with each other's** applications without a host computer. It is a message-based protocol, designed originally for multiplex electrical **wiring within automobiles** to save on copper, but it can also be used in many other contexts. For each device, the data in a frame is transmitted serially but in such a way that if more than one device transmits at the same time, the highest priority device can continue while the others back off. Frames are received by all devices, including by the transmitting device. (Source: Wikipedia.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a nutshell, is a communications bus for vehicle sensors and systems (aka. ECUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Know your Data: J1939 & CAN\n",
    "\n",
    "### What is the J1939 Standard?\n",
    "\n",
    "Society of Automotive Engineers standard **SAE J1939** is the vehicle bus recommended practice used for communication and diagnostics among vehicle components. Originating in the car and heavy-duty truck industry in the United States, it is now widely used in other parts of the world. SAE J1939 is **used in the commercial vehicle area for connection and communication throughout the vehicle**, with the physical layer defined in **ISO 11898 (aka. CAN)**. A different physical layer is used between the tractor and trailer, specified in ISO 11992. (Lightly edited from Wikipedia.org)\n",
    "\n",
    "### J1939 Messages\n",
    "\n",
    "A J1939 message contains two important fields: \n",
    "\n",
    "* PGN: Parameter Group Number, also known as Message ID, contain information about sender, topic, and priority. PGNs are sent \n",
    "* SPN: Suspect Parameter Numbers contain the \"data payload\". Payloads can contain various parameters (or signals) associated to a single PGN\n",
    "\n",
    "<img src=img/j1939-msg.png width=\"683\" height=\"90\">\n",
    "\n",
    "*The structure of a J1939 Message*\n",
    "\n",
    "<img src=img/j1939-can-sample.png width=\"530\" height=\"166\">\n",
    "\n",
    "*J1939 message samples recorded from a Corn Harvester in the 2021 Harvest Season in NE. Lots of Hexadecimal numbers! (Courtesy of Stevens Farms, NE)*\n",
    "\n",
    "### J1939 Messages: Parameter Group Numbers\n",
    "\n",
    "PGNs point to vehicle signals and parameters which are defined as Parameter Groups\n",
    "\n",
    "<img src=img/j1939-pgn.png width=\"837\" height=\"99\">\n",
    "\n",
    "* A paramater group can be \"engine temperature\", which contains signals related to the temperature of several components on the engine, such as coolant temperature, fuel temperature, oil temperature, etc.\n",
    "* PGNs are numbers that can range from 0 - 65279 (0-FEFF in hexadecimal)\n",
    "* In J1939 there are **hundreds** of definitions for parameter numbers\n",
    "\n",
    "### J1939 Messages: Suspect Parameter Numbers\n",
    "\n",
    "SPNs are numbers assigned by the SAE to a specific parameter within a parameter group.\n",
    "\n",
    "<img src=img/j1939-spn.png width=\"697\" height=\"225\">\n",
    "\n",
    "* SPNs contained within a parameter group share common characteristics (**a context**)\n",
    "* Data payloads contain several SPNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Data Wrangling with Pandas\n",
    "Now that we know more about our dataset, let's dive in and do some wrangling! We are going to work with a small data log recorded from a John Deere 320D skid-steer machine using an ISOBlue edge-computer. Data was recorded during the CAN BUS session of the Spring 2022 session of the ASM345 (Power Units and Power Trains) course at Purdue. It contains all sorts of data from machine sensors (some unknown to people... outside OEMs). For this exercise, we are going to use **Python, Numpy, Matplotlib, and Pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data processing libraries imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have pytables installed before running this notebook. If you don't have it, just run *python3 -m pip install tables* in your computer's commandline.\n",
    "\n",
    "First we start declaring the dataset's metadata (file path, event, machine under test, and date). The metadata will allow us to easily locate the file in case of analyzing multiple experiments/events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Declare raw log path \n",
    "\n",
    "DATA_PATH = 'data/'\n",
    "\n",
    "## Declare event, machine, and date from log\n",
    "\n",
    "EVENT = 'asm345'\n",
    "MACHINE_ID = 'j320d'\n",
    "DATE = '04132022'\n",
    "\n",
    "## Generate filename based on experiment event, machine ID and date\n",
    "\n",
    "CSV_NAME = '-'.join([EVENT, MACHINE_ID, DATE, 'tra.csv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to read our data from our dataset file. Use pandas to read data from our CSV file. **Hint:** The Path and the CSV name will give you the full path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always a good idea to declare the data type of each column of our data set before beginning our analysis. We do it with a Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'time': str, \\\n",
    "    'can_interface': str, \\\n",
    "    'can_id': str, \\\n",
    "    'can_data': str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(dtypes.keys()) ## Parse our dictionary to a list containing the name of our columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_col = [cols[0]] ## Select the first column as our time index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(can_log['time'][0]) ## Check the data type of the rows in the 'time' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How big is our data set?  \n",
    "\n",
    "It's always a good practice to look at the dimension of the data we are going to work on. That allows us to tell what we can and can't do (in terms of efficient computing).\n",
    "Use pandas to return the number of rows in our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data clean up\n",
    "\n",
    "Data sets can have hundreds of thousands of rows, but some of them are invalid entries. It is always good to discard those invalid entries before performing any operation on the data set. Use Pandas to discard rows with invalid timestamps (entries in the 'time' column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timestamps and time zones\n",
    "For clarity, most data collection systems and loggers use UTC (Universal Time Coordinated) timestamps. This avoids confusions due to ambiguous timezones, dayligth savings time. Though, it is hard to correlate with our clocks, which follow a timezone. We can use Pandas to change the timezone of our dataframe timestamps. Create a new column with name **ts_corr** for our corrected timestamps.\n",
    "\n",
    "**Hint:** Use Pandas *.apply()* and an embedded **lambda** function to apply the change to all the rows of the dataframe. \n",
    "\n",
    "* https://www.dataquest.io/blog/tutorial-lambda-functions-in-python/\n",
    "* https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check again the dataframe to verify our operation above was correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember:** We are working with **time series data**, which means that data points are **indexed in time order**. Use Pandas to set the corrected timestamps (**ts_corr**) as the dataframe index, and discard the unnecesary columns of the data set. Also, it is a good idea to give columns more intuitive names, based on context. Use Pandas to give better names to the columns of the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check again the contents of your data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a cleaner dataframe, it is time to start working on our data. Below you will find a helper function: **parse_canid(can_id)**. This function will extract the PGNs (Parameter Group Number), SA (Source Address), DA (Destination Address), and PRIO (Priority) of the recorded messages in our log from the **can_id** column. It takes a 4-byte **hexadecimal** number as input and returns 4 elements, each one a **decimal** number representing the PGN, SA, DA, and PRIO embedded in the CAN ID. \n",
    "\n",
    "**Some useful readings:**\n",
    "* https://en.wikipedia.org/wiki/Hexadecimal\n",
    "* https://en.wikipedia.org/wiki/Byte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISOBUS message masks\n",
    "MASK_2_BIT = ((1 << 2) - 1)\n",
    "MASK_3_BIT = ((1 << 3) - 1)\n",
    "MASK_8_BIT = ((1 << 8) - 1)\n",
    "\n",
    "def parse_canid(can_id):\n",
    "    # J1939 header info:\n",
    "    # http://www.ni.com/example/31215/en/\n",
    "    # http://tucrrc.utulsa.edu/J1939_files/HeaderStructure.jpg\n",
    "    if can_id is None:\n",
    "        return [-1] * 4\n",
    "    \n",
    "    header = int(can_id, 16)\n",
    "\n",
    "    sa = header & MASK_8_BIT\n",
    "    header >>= 8\n",
    "    pdu_ps = header & MASK_8_BIT\n",
    "    header >>= 8\n",
    "    pdu_pf = header & MASK_8_BIT\n",
    "    header >>= 8\n",
    "    res_dp = header & MASK_2_BIT\n",
    "    header >>= 2\n",
    "    priority = header & MASK_3_BIT\n",
    "\n",
    "    pgn = res_dp\n",
    "    pgn <<= 8\n",
    "    pgn |= pdu_pf\n",
    "    pgn <<= 8\n",
    "    if pdu_pf >= 240:\n",
    "        # pdu format 2 - broadcast message. PDU PS is an extension of\n",
    "        # the identifier\n",
    "        pgn |= pdu_ps\n",
    "        da = 255\n",
    "    else:\n",
    "        da = pdu_ps\n",
    "        \n",
    "    return [pgn, sa, da, priority]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add **four** new columns with names **pgn**, **sa**, **da**, **pri** to the dataframe and check the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fil the newly created columns with parsed data from the **can_id** column. Use the **parse_canid()** function to parse each individual id and assign the results to each column. \n",
    "\n",
    "**Hint:** You can make a partition from the dataframe, selecting the columns with empty values and then assign the result of **parse_canid()** to each row. Remember, this function returns a list of **4** elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the dataframe after parsing the CAN IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering data by PGN\n",
    "\n",
    "There are hundreds of PGNs in our log, some are well documented and available to the public as part of the J1939 standard, and others hold proprietary information that only OEMs know how to decode. For this exercise, we will focus on three PGNs: **61444**, **65266**, and **65128**, corresponding to the Electronic Engine Controller #1, Engine Fuel Rate, and Hydraulic Temp. Controller respectively. Below you will find a dictionary that contains the definition of each PGN, each one containing another dictionary (this is called a **nested dictionary**), where the **bytes** key specifies the portion of the payload that contains the payload of interest, **offset** includes a linear offset to the obtained value, **resolution** is a scaling constant for each signal, **name** and **unit** correspond to the name and units of the obtained signal. \n",
    "\n",
    "Use this dictionary to decode the data from the signals contained in each message payload.\n",
    "\n",
    "**Looking for a challenge? (OPTIONAL CHALLENGE!)**\n",
    "Too few signals? You can add your own signal definitions! Look on the internet and add more signal definitions to the **pgns** dictionary. Be careful, not all signals are present on our data set. Some signals correspond to systems and sensors not present in the machine under test (skid-steer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgns = {\n",
    "    61444: {'bytes': list(range(32,40)) + list(range(24,32)), 'offset': 0, 'resolution': 0.125, 'name': 'Engine RPM', 'unit': 'RPM'},\n",
    "    65266: {'bytes': list(range(8,16)) + list(range(0,8)), 'offset': 0, 'resolution': 0.05, 'name': 'Fuel Rate', 'unit': 'L/h'},\n",
    "    65128: {'bytes': range(0,8), 'offset': -40.0, 'resolution': 1.0, 'name': 'Hydraulic Temp', 'unit': 'degree C'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have two helper functions: **hex2bin()** which converts a hexadecimal number in *string form* to a binary number in *string form*; and **map2col()** maps values from a binary number in *string form* to a column for each binary digit (e.g., 01010 will be mapped to five columns, each one containing a digit of the number 01010). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex2bin(hex_str):\n",
    "    scale = 16 # equals to hexadecimal\n",
    "    num_of_bits = 64\n",
    "    return str(bin(int(hex_str, scale))[2:].zfill(num_of_bits))\n",
    "\n",
    "def map2col(n, payload_bin_str):\n",
    "    bin_vals = list(payload_bin_str)\n",
    "    return bin_vals[n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to decode the signals. Use the **pgns** dictionary and the two helper functions above to decode the signal from the PGN. We will do it in a few steps:\n",
    "\n",
    "1. **for** each PGN, convert the payload from hexadecimal to a binary. Then map each digit to an individual column into an auxiliary dataframe (avoid adding the new columns to the dataframe of our data set.\n",
    "2. After mapping, use the bytes key of the PGN definitions dictionary to extract the desired signal from the payload. Then convert the result to a decimal number number and use the formula below to properly scale the signal:\n",
    "\n",
    "$$ y_{sig}(x) = x*resolution + offset $$ \n",
    "\n",
    "Where $x$ is the decimal number obtained after filtering and converting the payload, $resolution$ and $offset$ can be extracted from the PGNs definition dictionary.\n",
    "\n",
    "3. **Apply** the formula for each PGN. Then save to a new dataframe, and plot the results using **matplotlib**. Give some cool styles to your plots!\n",
    "\n",
    "**Hints:** \n",
    "* Calling the helper functions as **lambda** functions and using them inside an **apply()** function over a dataframe will help you **a lot** \n",
    "* You can convert from a binary number in *string form* to a decimal number using the **int(n, 2)** function, where **n** is a binary number in *string form*, and 2 represents the base of the number system (2 for binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all folks! I hope you had fun doing some data wrangling with data from real sensors. If you have any questions, or found any oversight in this workshop, please send me an email (fcastibl@purdue.edu) and I can help you with any questions. Also, corrections are always greatly appreciated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
